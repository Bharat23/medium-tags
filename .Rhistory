# This function has already been implemented for you. It takes the data matrix and method name, outputs the distance
# matrix based on the method name.
distance_matrix = matrix(0L, nrow = nrow(test_matrix), ncol = nrow(train_matrix))
# the looping logic for pairwise distances is already provided for you
for(i in seq(1, nrow(test_matrix))){
for(j in seq(1, nrow(train_matrix))){
distance_matrix[i,j] <- do.call(method_name, list(unlist(test_matrix[i,]), unlist(train_matrix[j,])))
}
}
return(distance_matrix)
}
calculate_euclidean <- function(p, q) {
# Input: p, q are vectors of size 1 x 100, each representing a row (i.e., a sentence) from the original dataset.
# output: a single value of type double, containing the euclidean distance between the vectors p and q
# Write code here to calculate the euclidean distance between pair of vectors p and q
difference = p-q;
differenceSquare = (difference)^2;
differenceSquareSum = sum(differenceSquare);
differenceSquareSumRoot = sqrt(differenceSquareSum);
return(differenceSquareSumRoot);
}
calculate_cosine <- function(p, q) {
# Input: p, q are vectors of size 1 x 100, each representing a row (i.e., a sentence) from the original dataset.
# output: a single value of type double, containing the cosine distance between the vectors p and q
# Write code here to calculate the cosine distance between pair of vectors p and q
product = p*q;
productSum = sum(product);
pMod = calculate_vector_mod(p);
qMod = calculate_vector_mod(q);
modProduct = pMod * qMod;
cosineDistance = productSum/modProduct;
return(cosineDistance);
}
calculate_vector_mod <- function (vec) {
# function to calculate the mod of the vector
# Input vector of any length
# return the mod of the vector
vectorProduct = vec*vec;
vectorProductSum = sum(vectorProduct);
vectorProductSumRoot = sqrt(vectorProductSum);
return(vectorProductSumRoot);
}
knn_classifier <- function(x_train, y_train, x_test, distance_method, k){
# You will be IMPLEMENTING a KNN Classifier here
# Build a distance matrix by computing the distance between every test sentence
# (row in training TF-IDF matrix) and training sentence (row in test TF-IDF matrix).
# Use the above calculate_distance_matrix function to calculate this distance matrix (code already given to you).
# You can re-use the calculate_euclidean and calculate_cosine methods from HW1 here.
# Once the distance matrix is computed, for each row in the distance matrix, calculate the 'k' nearest neighbors
# and return the most frequently occurring class from these 'k' nearest neighbors.
# INPUT:
# x_train: TF-IDF matrix with dimensions: (number_training_sentences x number_features)
# y_train: Vector with length number_training_sentences of type factor - refers to the class labels
# x_test: TF-IDF matrix with dimensions: (number_test_sentences x number_features)
# k: integer, represents the 'k' to consider in the knn classifier
# distance_method: String, can be of type ('calcualte_euclidean' or 'calculate_cosine')
# OUTPUT:
# A vector of predictions of length = number of sentences in y_test and of type factor.
# NOTE 1: Don't normalize the data before calculating the distance matrix
# NOTE 2: For cosine, remember, you are calculating similarity, not distance. As a result, K nearest neighbors
# k values with highest values from the distance_matrix, not lowest.
# For euclidean, you are calculating distance, so you need to consider the k lowest values.
# NOTE 3:
# In case of conflicts, choose the class with lower numerical value
# E.g.: in 5NN, if you have 2 NN of class 1, 2 NN of class 2, and 1 NN of class 3, there is a conflict b/w class 1 and class 2
# In this case, you will choose class 1.
# NOTE 4:
# You are not allowed to use predefined knn-based packages/functions. Using them will result in automatic zero.
# Allowed packages: R base, utils
# calculate distance matrix
distance_matrix = calculate_distance_matrix(x_train, x_test, distance_method);
# initialize the output vector
output = vector(mode = "double", length = nrow(distance_matrix));
# traverse the distance matrix row wise finding the top k similar sentences
for (i in seq(nrow(distance_matrix))) {
# fetches the ith row of the distance matrix
thisRow = distance_matrix[i,];
# sort the row and return their indexes
sortedIndexes = sort(thisRow, index.return=TRUE)$ix
# based on selected distance method return the top k values
# for euclidean distance we consider k smallest values
# for cosine distance we consider k largest values
if (distance_method == "calculate_euclidean") {
knnIndex = head(sortedIndexes, k)
} else if (distance_method == "calculate_cosine") {
knnIndex = tail(sortedIndexes, k)
}
# return the classes of the corresponding to the indexes
knns = y_train[knnIndex];
# sort the indexes
# this comes in handy when there is a collision between knn with same frequency
# this ensures the smallest value is returned
knnsSorted = sort(knns);
# pick out the uniques classes
# made dynamic to avoid hard coding
knnsClasses = unique(knnsSorted);
# match function returns the first index of the matched values
firstIndexes = match(knnsSorted, knnsClasses);
# calculates the frequency and arrange in form of tables
frequencyTable = tabulate(firstIndexes);
# return the index with highest frequency
highestFrequency = which.max(frequencyTable);
# based on the index return pick the class
thisClass = knnsClasses[highestFrequency];
# set the class for the ith position in the output vector
output[i] = thisClass;
}
# returns the output of type factor
factor(output);
}
knn_classifier_confidence <- function(x_train, y_train, x_test, distance_method='calculate_cosine', k){
# You will be trying to build a modified KNN classifier using the paper given in the HW
# While most of the implementation is similar to the KNN classifier, there is one additional step you need to do.
# Read the HW PDF for more details about this method
# INPUT:
# x_train: TF-IDF matrix with dimensions: (number_training_sentences x number_features)
# y_train: Vector with length number_training_sentences of type factor - refers to the class labels
# x_test: TF-IDF matrix with dimensions: (number_test_sentences x number_features)
# k: integer, represents the 'k' to consider in the knn classifier
# distance_method: String, can be of type ( 'calculate_cosine')
# OUTPUT:
# A vector of predictions of length = number of sentences in y_test and of type factor.
# Read the NOTES from comments under knn_classifier.
# Allowed packages: R base, utils
# calculate distance matrix
distance_matrix = calculate_distance_matrix(x_train, x_test, distance_method);
distance_matrix_cosine = calculate_distance_matrix(x_train, x_test, "calculate_cosine");
# initialize the output vector
output = vector(mode = "double", length = nrow(distance_matrix));
# traverse the distance matrix row wise finding the top k similar sentences
for (i in seq(nrow(distance_matrix))) {
# fetches the ith row of the distance matrix
thisRow = distance_matrix[i,];
sumTotalK = sum(thisRow);
# sort the row and return their indexes
sortedIndexes = sort(thisRow, index.return=TRUE)$ix
# based on selected distance method return the top k values
# for euclidean distance we consider k smallest values
# for cosine distance we consider k largest values
if (distance_method == "calculate_euclidean") {
knnIndex = head(sortedIndexes, k)
} else if (distance_method == "calculate_cosine") {
knnIndex = tail(sortedIndexes, k)
}
# return the classes of the corresponding to the indexes
knns = y_train[knnIndex];
sums = vector(mode="double", length = 4)
totalSum = 0;
# calculate the total sum and sum per class
for (j in seq(1,k)) {
sums[knns[j]] = sums[knns[j]] + thisRow[knnIndex[j]];
totalSum = totalSum + thisRow[knnIndex[j]];
}
maxConfidence = 0;
maxConfidenceIndex = 0;
# calculate classwise confidence
for (j in seq(1,4)) {
tmp = sums[j]/totalSum;
if (tmp > maxConfidence) {
maxConfidence = tmp;
maxConfidenceIndex = j;
}
}
output[i] = maxConfidenceIndex;
# sort the indexes
# this comes in handy when there is a collision between knn with same frequency
# this ensures the smallest value is returned
#knnsSorted = sort(knns);
# pick out the uniques classes
# made dynamic to avoid hard coding
#knnsClasses = unique(knnsSorted);
# match function returns the first index of the matched values
#firstIndexes = match(knnsSorted, knnsClasses);
# calculates the frequency and arrange in form of tables
#frequencyTable = tabulate(firstIndexes);
# return the index with highest frequency
#highestFrequency = which.max(frequencyTable);
# based on the index return pick the class
#thisClass = knnsClasses[highestFrequency];
#print(thisRow);
#print(thisClass)
# set the class for the ith position in the output vector
#output[i] = thisClass;
}
# returns the output of type factor
factor(output);
}
dtree <- function(x_train, y_train, x_test){
set.seed(123)
# You will build a CART decision tree, then use the tuned model to predict class values for a test dataset.
# INPUT:
# x_train: TF-IDF matrix with dimensions: (number_training_sentences x number_features)
# y_train: Vector with length number_training_sentences of type factor - refers to the class labels
# x_test: TF-IDF matrix with dimensions: (number_test_sentences x number_features)
# OUTPUT:
# A vector of predictions of length = number of sentences in y_test and of type factor.
# Allowed packages: rpart, R Base, utils
# HINT1: Make sure to read the documentation for the rpart package. Check out the 'rpart' and 'predict' functions.
# HINT2: I've given you attributes and class labels as separate variables. Do you need to combine them
# into a data frame for rpart?
data = as.data.frame(cbind(x_train, y_train));
model = rpart(formula=y_train~.,data = data, method = "class")
testData = as.data.frame(x_test);
predictions = predict(model, testData, type="class")
predictions
}
dtree_cv <- function(x_train, y_train, x_test, n_folds){
set.seed(123)
# You will build a decision tree and tune its parameters using n-fold crossvalidation on the *training* dataset,
# then use the tuned model to predict class values for a test dataset.
# INPUT:
# x_train: TF-IDF matrix with dimensions: (number_training_sentences x number_features)
# y_train: Vector with length number_training_sentences of type factor - refers to the class labels
# x_test: TF-IDF matrix with dimensions: (number_test_sentences x number_features)
# n_folds: integer, refers to the number of folds for n-fold cross validation
# OUTPUT:
# A vector of predictions of length = number of sentences in y_test and of type factor.
# Allowed packages: rpart, caret, R Base, utils
# HINT1: Make sure to read the documentation for the caret package. Check out the 'train' and 'trainControl' functions.
data = as.data.frame(cbind(x_train, y_train));
model = train(y_train~., data = data, method="rpart", trControl=trainControl(method="cv", number = n_folds));
testData = as.data.frame(x_test);
#prediction = predict(model, testData);
#prediction;
factor(predict(object=model, newdata = x_test))
model
}
calculate_accuracy <- function(y_pred, y_true){
# Given the following:
# INPUT:
# y_pred: predicted class labels (vector, each value of type factor)
# y_true: ground truth class labels (vector, each value of type factor)
# OUTPUT:
# a list in the following order: [confusion matrix, overall accuracy], where confusion matrix is of class "table"
# (see Figure 2 in the PDF for an example Confusion Matrix)
# and overall accuracy is on a scale of 0-1 of type double
# overall class accuracy = accuracy of all the classes
# confusion matrix should have Prediction to the left, and Reference on the top.
confusionMatrix = table(y_pred, y_true, dnn = c('Prediction', 'Reference'));
numerator = 0;
denominator = 0;
for (i in seq(1,ncol(confusionMatrix))) {
for (j in seq(1, nrow(confusionMatrix))) {
if (i == j) {
numerator = numerator + confusionMatrix[i,j];
}
denominator = denominator + confusionMatrix[i,j];
}
}
overallAccuracy = numerator/denominator;
list("confusion_matrix"=confusionMatrix, "overall_accuracy"=overallAccuracy)
}
# read data
training = read_data('./hw2_training.csv');
test = read_data('./hw2_test.csv');
# call knn classifier
#cosine = knn_classifier(training[,-101], training[,101], test[,-101], "calculate_cosine", 5);
#euclidean = knn_classifier(training[,-101], training[,101], test[,-101], "calculate_euclidean", 5);
#knn_classifier_confidence(training[,-101], training[,101], test[,-101], "calculate_cosine", 5);
#matrix_cosine = calculate_distance_matrix(training[,-101], test[,-101], "calculate_cosine")
#pred = dtree(training[,-101], factor(training[,101]), test[,-101]);
tree = dtree_cv(training[,-101], factor(training[,101]), test[,-101], 10);
pp = calculate_accuracy(c(1,2,3,4,4), c(1,2,4,3,4));
caret:::confusionMatrix(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)), dnn=c('a', 'b'))
#plot(tree)
#par(xpd = NA)
#text(tree,  digits = 2)
pp = calculate_accuracy(c(1,2,3,4,4), c(1,2,4,3,4));
caret:::confusionMatrix(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)), dnn=c('a', 'b'))
require(e1071)
install.packages("e1071")
pp = calculate_accuracy(c(1,2,3,4,4), c(1,2,4,3,4));
caret:::confusionMatrix(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)), dnn=c('a', 'b'))
caret:::confusionMatrix(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)), dnn=c('Prediction', 'Reference'))
test = caret:::confusionMatrix(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)), dnn=c('Prediction', 'Reference'))
test$table
test$overall
test$overall[]
test$overall[1,]
test$overall['Accuracy']
test$overall['Accuracy', 1]
test$overall[1, 'Accuracy']
test$overall['Accuracy']
test$overall['Accuracy'][1]
test$overall['Accuracy', 0]
type(test$overall['Accuracy'])
typeof(test$overall['Accuracy'])
type(test$overall['Accuracy'])
test$overall['Accuracy']
test$overall['Accuracy':1]
test$overall['Accuracy',1,1,]
test$overall['Accuracy']
test$overall
typeof(test$overall)
a = test$overall
a = test$overall[['Accuracy']]
a
calculate_accuracy <- function(y_pred, y_true){
# Given the following:
# INPUT:
# y_pred: predicted class labels (vector, each value of type factor)
# y_true: ground truth class labels (vector, each value of type factor)
# OUTPUT:
# a list in the following order: [confusion matrix, overall accuracy], where confusion matrix is of class "table"
# (see Figure 2 in the PDF for an example Confusion Matrix)
# and overall accuracy is on a scale of 0-1 of type double
# overall class accuracy = accuracy of all the classes
# confusion matrix should have Prediction to the left, and Reference on the top.
confusionMatrix = caret:::confusionMatrix(y_pred, y_true, dnn=c('Prediction', 'Reference'))
list("confusion_matrix"=confusionMatrix$table, "overall_accuracy"=confusionMatrix$overall[['Accuracy']])
}
pp = calculate_accuracy(c(1,2,3,4,4), c(1,2,4,3,4));
pp = calculate_accuracy(factor(c(1,2,3,4,4)), factor(c(1,2,4,3,4)));
pp
list("test"=23)
rm(list = ls(all = T))
# set working directory
#setwd('./')
source('./hw2.R')
# function to load and process training and test data
# Please note that TA may have a completely different dataset with the same dimensions as the one provided to you
load_train_and_test_data <- function(folder_path='./'){
# Input: folder_path - points to the folder containing the hw2_training and hw2_test csv files
# TA has different versions the same file, following the same properties of the data (# row, # columns and class values {1,2,3,4})
tr_data <- read.csv(paste0(folder_path, 'hw2_training.csv'), stringsAsFactors= T)
te_data <- read.csv(paste0(folder_path, 'hw2_test.csv'), stringsAsFactors= T)
X_train <- tr_data[, 1:100]
X_test <- te_data[, 1:100]
y_train <- as.factor(tr_data[, 101])
y_test <- as.factor(te_data[, 101])
return(list(X_train, X_test, y_train, y_test))
}
# read data from disk, extract train test into separate variables
all_data <- load_train_and_test_data('./')
X_train <- all_data[[1]]
X_test <- all_data[[2]]
y_train <- all_data[[3]]
y_test <- all_data[[4]]
# calculate classification outcomes using KNN with euclidean distance
euclidean_classification <- knn_classifier(X_train, y_train, X_test, 'calculate_euclidean', 5)
# calculate classification outcomes using KNN with cosine distance
cosine_classification <- knn_classifier(X_train, y_train, X_test, 'calculate_cosine', 5)
# calculate classification outcomes using KNN_V2 with cosine distance
knn_conf_classification <- knn_classifier_confidence(X_train, y_train, X_test,  'calculate_cosine', 5)
# calculate classification outcomes using Decision Tree using rpart and gini index with default hyperparameters
dt_classification <- dtree(X_train, y_train, X_test, 5)
# calculate classification outcomes using a tuned Decision Tree
dt_cv_classification <- dtree_cv(X_train, y_train, X_test, 5)
euclidean_result <- calculate_accuracy(euclidean_classification, y_test)
cosine_result <- calculate_accuracy(cosine_classification, y_test)
conf_result <- calculate_accuracy(knn_conf_classification, y_test)
dt_result <- calculate_accuracy(dt_classification, y_test)
dt_cv_result <- calculate_accuracy(dt_cv_classification, y_test)
rm(list = ls(all = T))
source('./hw2.R')
rm(list = ls(all = T))
source('./hw2.R')
getwd()
rm(list = ls(all = T))
source('./hw2.R')
getwd()
getwd()
#plot(tree)
#par(xpd = NA)
#text(tree,  digits = 2)
getwd()
rm(list = ls(all = T))
source('./hw2.R')
getwd()
getwd()
getwd()
setwd(paste(getwd(), "/code/medium", sep = ""))
getwd()
install.packages('text2vec');
require('text2vec');
data_new = read.csv('./Data_1_1000.csv', stringsAsFactors = FALSE);
data_new['content']
data_new[, 12]
colnames(data_new)
data = data_new[, c(12, 13, 15:109)]
filtered_data = data.frame(Tag=character(), AvgRead=float(), claps=float(), visibility=float(), ratio=float(), stringsAsFactors = FALSE)
filtered_data = data.frame(Tag=character(), AvgRead=double(), claps=double(), visibility=double(), ratio=double(), stringsAsFactors = FALSE)
for (i in seq(3, ncol(data_new))) {
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
cNames = colnames(data);
data = data_new[, c(12, 13, 15:109)]
cNames = colnames(data);
filtered_data = data.frame(Tag=character(), AvgRead=double(), claps=double(), visibility=double(), ratio=double(), stringsAsFactors = FALSE)
for (i in seq(3, ncol(data_new))) {
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
for (i in seq(3, ncol(data_new))) {
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
#filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
index = which(data[, 2] == 1);
for (i in seq(3, ncol(data_new))) {
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
#filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
for (i in seq(3, ncol(data_new))) {
index = which(data[, i] == 1);
#sum = sum(data[index, 2]);
#count = length(index)
#ratio = sum/count;
#avg = sum(data[index, 1])/count;
#filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
for (i in seq(3, ncol(data_new))) {
print(i)
index = which(data[, i] == 1);
#sum = sum(data[index, 2]);
#count = length(index)
#ratio = sum/count;
#avg = sum(data[index, 1])/count;
#filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
for (i in seq(3, ncol(data))) {
print(i)
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
View(filtered_data)
View(filtered_data)
View(filtered_data)
model = lm(filtered_data[, 'AvgRead']~filtered_data[, 'ratio'])
plot(model)
model
plot(filtered_data[, 'AvgRead']~filtered_data[, 'ratio'])
plot(filtered_data[, 'AvgRead'], filtered_data[, 'ratio'])
View(filtered_data)
model = lm(filtered_data[, 'ratio']~filtered_data[, 'AvgRead'])
plot(filtered_data[, 'ratio'], filtered_data[, 'AvgRead'])
model = lm(filtered_data[, 'AvgRead'], filtered_data[, 'ratio'])
coeff=coefficients(reg)
coeff=coefficients(model)
abline(coef = coeff)
library(udpipe)
model <- udpipe_download_model(language = "english")
data_for_dist = data_new[, 15:109]
jaccard_dist = dist(data_for_dist, method="jaccard")
jaccard_dist = distance(data_for_dist, method="jaccard")
library(philentropy)
data_for_dist = data_new[, 15:109]
jaccard_dist = distance(data_for_dist, method="jaccard")
model = hclust(jaccard_dist, method = "single")
kmeans(data_for_dist)
kmeans(data_for_dist, algorithm = "Llyod")
kmeans(data_for_dist, centers = c(0) algorithm = "Llyod")
kmeans(data_for_dist, centers = 2 algorithm = "Llyod")
kmeans(data_for_dist, centers = 2, algorithm = "Llyod")
km = kmeans(data_for_dist, centers = 2, algorithm = "Lloyd")
plot(km$cluster)
?sd
sd(filtered_data[, 'ratio'])
sd(filtered_data[, 'ratio'], na.rm = TRUE)
meanRatio = mean(filtered_data[, 'ratio']);
(filtered_data[, 'ratio'] - meanRatio)/sd(filtered_data[, 'ratio'], na.rm = TRUE)
meanRatio = mean(filtered_data[, 'ratio'], na.rm = TRUE);
(filtered_data[, 'ratio'] - meanRatio)/sd(filtered_data[, 'ratio'], na.rm = TRUE)
filtered_data[, 'ratio'] = (filtered_data[, 'ratio'] - meanRatio)/sd(filtered_data[, 'ratio'], na.rm = TRUE)
plot(filtered_data[, 'ratio'])
View(filtered_data)
View(filtered_data)
plot(filtered_data[, 'ratio'])
hist(filtered_data[, 'ratio'])
filtered_data = data.frame(Tag=character(), AvgRead=double(), claps=double(), visibility=double(), ratio=double(), stringsAsFactors = FALSE)
for (i in seq(3, ncol(data))) {
print(i)
index = which(data[, i] == 1);
sum = sum(data[index, 2]);
count = length(index)
ratio = sum/count;
avg = sum(data[index, 1])/count;
filtered_data[nrow(filtered_data) + 1,] = list(Tag=cNames[i], AvgRead=avg, claps=sum, visibility=count, ratio=sum/count)
}
meanRatio = mean(filtered_data[, 'ratio'], na.rm = TRUE);
filtered_data[, 'normal'] = (filtered_data[, 'ratio'] - meanRatio)/sd(filtered_data[, 'ratio'], na.rm = TRUE)
hist(filtered_data[, 'ratio'])
View(filtered_data)
View(filtered_data)
normalizer = function (data) {
meanRatio = mean(data, na.rm = TRUE);
(data - meanRatio)/sd(data, na.rm = TRUE)
}
filtered_data[, 'normalClaps'] = normalizer(filtered_data[, 'claps']);
filtered_data[, 'normalVisibility'] = normalizer(filtered_data[, 'visibility']);
View(filtered_data)
View(filtered_data)
