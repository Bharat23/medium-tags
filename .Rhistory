# clear workspace
rm(list=ls(all=T))
cat('\014') # clear console
# source hw3.R
source('./hw4.R')
# install all necessary packages
required_packages = c("caret", "nnet", "stats", "dplyr", "ggplot2")
for(package in required_packages){
if(!(package %in% installed.packages())){
install.packages(package, dependencies = T)
}
}
# load the packages
library('caret') # for cross validation
library('nnet') # for neural networks
library('stats') # for clustering
library('dplyr') # if needed
library('ggplot2') # for plotting elbow plot
# set seed
set.seed(100)
############################################################################################################
# Helper functions
# TA will use something similar to load data for his own system
# For regression data
load_data <- function(data_folder='./data/', learning_type){
# this method will read data for clustering/classification and return list containing two data frames:
# for clustering (specified by learning_type = "clustering")
# two columns (x, y) of continous data (of type double)
# for classification (specified by learning_type = "classification")
# first 4 columns (x1-x4) are attributes, last column (class) is your dependent variable (factor))
# for classification, please note, TA WILL use the same training dataset, but a different test set
# TA's test set will have the same attributes (x1-x4, class), but may contain different number of data points
# for clustering, please note, TA will use a different dataset
# make sure dependent variable is of type factor if this is classification
if(learning_type == 'classification'){
train_df <- read.csv(paste0(data_folder, learning_type, '-train.csv'), header=T)
test_df <- read.csv(paste0(data_folder, learning_type, '-test.csv'), header=T)
train_df$class <- as.factor(train_df$class)
test_df$class <- as.factor(test_df$class)
return(list(train_df, test_df))
}else{
data_df <- read.csv(paste0(data_folder, learning_type, '_sample.csv'), header = T)
}
}
##########################################################################################################
# Load data
# load data necessary for clustering
clustering_data <- load_data(data_folder='./data/', learning_type='clustering')
# load data necessary for classification
clf_data <- load_data(data_folder='./data/', learning_type='classification')
clf_train_df <- clf_data[[1]]
clf_test_df <- clf_data[[2]]
# KMeans
kmeans_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "kmeans")
kmeans_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = kmeans_result)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
single_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = single_link_result)
# complete link
complete_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "complete-link")
complete_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = complete_link_result)
# Setup for analysis section in clustering
# generate the elbow plot for kmeans for c(1, 2, 3, 4, 5, 6, 7)
alda_kmeans_elbow_plot(data_df = clustering_data, k_values = c(1, 2,3,4,5,6,7))
# Code's already been written, no need to make any changes here
# First, lets evaluate the SSE values by printing them
print(paste("Kmeans SSE for given params = ", kmeans_sse))
print(paste("Single link SSE for given params = ", single_link_sse))
print(paste("Complete link SSE for given params = ", complete_link_sse))
# Next, lets evaluate visually by visualizing them
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=kmeans_result, main = "KMeans with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=single_link_result, main = "Single Link with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=complete_link_result, main = "Complete link with 2 clusters", xlab = 'x', ylab = 'y')
# create a parameter grid
parameter_grid <- expand.grid(size=c(4, 8, 16), decay = c(0.2, 0.5, 0.002, 0.005))
# perform grid search, predict on test set
nn_result <- alda_nn(x_train = clf_train_df[,-5], x_test = clf_test_df[, -5],
y_train = clf_train_df[,5],
parameter_grid = parameter_grid)
nn_result
# source hw3.R
source('./hw4.R')
# perform grid search, predict on test set
nn_result <- alda_nn(x_train = clf_train_df[,-5], x_test = clf_test_df[, -5],
y_train = clf_train_df[,5],
parameter_grid = parameter_grid)
nn_result
print(paste("Accuracy of my NN model = ", confusionMatrix(nn_result[[2]], clf_test_df[, 5])$overall['Accuracy']))
# source hw3.R
source('./hw4.R')
# perform grid search, predict on test set
nn_result <- alda_nn(x_train = clf_train_df[,-5], x_test = clf_test_df[, -5],
y_train = clf_train_df[,5],
parameter_grid = parameter_grid)
print(paste("Accuracy of my NN model = ", confusionMatrix(nn_result[[2]], clf_test_df[, 5])$overall['Accuracy']))
print("Confusion Matrix of my NN model: ")
print(confusionMatrix(nn_result[[2]], clf_test_df[, 5])$table)
# Code's already been written, no need to make any changes here
# First, lets evaluate the SSE values by printing them
print(paste("Kmeans SSE for given params = ", kmeans_sse))
print(paste("Single link SSE for given params = ", single_link_sse))
print(paste("Complete link SSE for given params = ", complete_link_sse))
# Next, lets evaluate visually by visualizing them
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=kmeans_result, main = "KMeans with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=single_link_result, main = "Single Link with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=complete_link_result, main = "Complete link with 2 clusters", xlab = 'x', ylab = 'y')
nn_result
print(paste("Accuracy of my NN model = ", confusionMatrix(nn_result[[2]], clf_test_df[, 5])$overall['Accuracy']))
x = c(5,1,4,9,2,6,8,2,6,3)
y = c(9,3,7,1,2,4,8,9,6,3)
d = dist(cbind(x,y), method = "euclidean", diag = TRUE, upper = TRUE, p = 2)
hc = hclust(d, method = "single", members = NULL)
cutree(hc, k = 3);
silhouette(clust)
install.packages('cluster')
require('cluster')
clust = cutree(hc, k = 3);
silhouette(clust)
silhouette(d)
silhouette(hc)
clust = cutree(hc, k = 3);
silhouette(clust)
silhouette(clust, d)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
single_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = single_link_result)
# source hw3.R
source('./hw4.R')
#############
# hw3 checker file
# Do not submit this file
#
#############
# clear workspace
rm(list=ls(all=T))
cat('\014') # clear console
# source hw3.R
source('./hw4.R')
# install all necessary packages
required_packages = c("caret", "nnet", "stats", "dplyr", "ggplot2")
for(package in required_packages){
if(!(package %in% installed.packages())){
install.packages(package, dependencies = T)
}
}
# load the packages
library('caret') # for cross validation
library('nnet') # for neural networks
library('stats') # for clustering
library('dplyr') # if needed
library('ggplot2') # for plotting elbow plot
# set seed
set.seed(100)
############################################################################################################
# Helper functions
# TA will use something similar to load data for his own system
# For regression data
load_data <- function(data_folder='./data/', learning_type){
# this method will read data for clustering/classification and return list containing two data frames:
# for clustering (specified by learning_type = "clustering")
# two columns (x, y) of continous data (of type double)
# for classification (specified by learning_type = "classification")
# first 4 columns (x1-x4) are attributes, last column (class) is your dependent variable (factor))
# for classification, please note, TA WILL use the same training dataset, but a different test set
# TA's test set will have the same attributes (x1-x4, class), but may contain different number of data points
# for clustering, please note, TA will use a different dataset
# make sure dependent variable is of type factor if this is classification
if(learning_type == 'classification'){
train_df <- read.csv(paste0(data_folder, learning_type, '-train.csv'), header=T)
test_df <- read.csv(paste0(data_folder, learning_type, '-test.csv'), header=T)
train_df$class <- as.factor(train_df$class)
test_df$class <- as.factor(test_df$class)
return(list(train_df, test_df))
}else{
data_df <- read.csv(paste0(data_folder, learning_type, '_sample.csv'), header = T)
}
}
##########################################################################################################
# Load data
# load data necessary for clustering
clustering_data <- load_data(data_folder='./data/', learning_type='clustering')
# load data necessary for classification
clf_data <- load_data(data_folder='./data/', learning_type='classification')
clf_train_df <- clf_data[[1]]
clf_test_df <- clf_data[[2]]
# KMeans
kmeans_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "kmeans")
kmeans_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = kmeans_result)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
single_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = single_link_result)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
#############
# hw3 checker file
# Do not submit this file
#
#############
# clear workspace
rm(list=ls(all=T))
cat('\014') # clear console
# source hw3.R
source('./hw4.R')
# install all necessary packages
required_packages = c("caret", "nnet", "stats", "dplyr", "ggplot2")
for(package in required_packages){
if(!(package %in% installed.packages())){
install.packages(package, dependencies = T)
}
}
# load the packages
library('caret') # for cross validation
library('nnet') # for neural networks
library('stats') # for clustering
library('dplyr') # if needed
library('ggplot2') # for plotting elbow plot
# set seed
set.seed(100)
############################################################################################################
# Helper functions
# TA will use something similar to load data for his own system
# For regression data
load_data <- function(data_folder='./data/', learning_type){
# this method will read data for clustering/classification and return list containing two data frames:
# for clustering (specified by learning_type = "clustering")
# two columns (x, y) of continous data (of type double)
# for classification (specified by learning_type = "classification")
# first 4 columns (x1-x4) are attributes, last column (class) is your dependent variable (factor))
# for classification, please note, TA WILL use the same training dataset, but a different test set
# TA's test set will have the same attributes (x1-x4, class), but may contain different number of data points
# for clustering, please note, TA will use a different dataset
# make sure dependent variable is of type factor if this is classification
if(learning_type == 'classification'){
train_df <- read.csv(paste0(data_folder, learning_type, '-train.csv'), header=T)
test_df <- read.csv(paste0(data_folder, learning_type, '-test.csv'), header=T)
train_df$class <- as.factor(train_df$class)
test_df$class <- as.factor(test_df$class)
return(list(train_df, test_df))
}else{
data_df <- read.csv(paste0(data_folder, learning_type, '_sample.csv'), header = T)
}
}
##########################################################################################################
# Load data
# load data necessary for clustering
clustering_data <- load_data(data_folder='./data/', learning_type='clustering')
# load data necessary for classification
clf_data <- load_data(data_folder='./data/', learning_type='classification')
clf_train_df <- clf_data[[1]]
clf_test_df <- clf_data[[2]]
# KMeans
kmeans_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "kmeans")
kmeans_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = kmeans_result)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
# complete link
complete_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "complete-link")
#############
# hw3 checker file
# Do not submit this file
#
#############
# clear workspace
rm(list=ls(all=T))
cat('\014') # clear console
# source hw3.R
source('./hw4.R')
# install all necessary packages
required_packages = c("caret", "nnet", "stats", "dplyr", "ggplot2")
for(package in required_packages){
if(!(package %in% installed.packages())){
install.packages(package, dependencies = T)
}
}
# load the packages
library('caret') # for cross validation
library('nnet') # for neural networks
library('stats') # for clustering
library('dplyr') # if needed
library('ggplot2') # for plotting elbow plot
# set seed
set.seed(100)
############################################################################################################
# Helper functions
# TA will use something similar to load data for his own system
# For regression data
load_data <- function(data_folder='./data/', learning_type){
# this method will read data for clustering/classification and return list containing two data frames:
# for clustering (specified by learning_type = "clustering")
# two columns (x, y) of continous data (of type double)
# for classification (specified by learning_type = "classification")
# first 4 columns (x1-x4) are attributes, last column (class) is your dependent variable (factor))
# for classification, please note, TA WILL use the same training dataset, but a different test set
# TA's test set will have the same attributes (x1-x4, class), but may contain different number of data points
# for clustering, please note, TA will use a different dataset
# make sure dependent variable is of type factor if this is classification
if(learning_type == 'classification'){
train_df <- read.csv(paste0(data_folder, learning_type, '-train.csv'), header=T)
test_df <- read.csv(paste0(data_folder, learning_type, '-test.csv'), header=T)
train_df$class <- as.factor(train_df$class)
test_df$class <- as.factor(test_df$class)
return(list(train_df, test_df))
}else{
data_df <- read.csv(paste0(data_folder, learning_type, '_sample.csv'), header = T)
}
}
##########################################################################################################
# Load data
# load data necessary for clustering
clustering_data <- load_data(data_folder='./data/', learning_type='clustering')
# load data necessary for classification
clf_data <- load_data(data_folder='./data/', learning_type='classification')
clf_train_df <- clf_data[[1]]
clf_test_df <- clf_data[[2]]
# KMeans
kmeans_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "kmeans")
kmeans_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = kmeans_result)
# Single link
single_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "single-link")
single_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = single_link_result)
# complete link
complete_link_result <- alda_cluster(data_df = clustering_data, n_clusters = 2, clustering_type = "complete-link")
complete_link_sse <- alda_calculate_sse(data_df = clustering_data, cluster_assignments = complete_link_result)
# Setup for analysis section in clustering
# generate the elbow plot for kmeans for c(1, 2, 3, 4, 5, 6, 7)
alda_kmeans_elbow_plot(data_df = clustering_data, k_values = c(1, 2,3,4,5,6,7))
# Code's already been written, no need to make any changes here
# First, lets evaluate the SSE values by printing them
print(paste("Kmeans SSE for given params = ", kmeans_sse))
print(paste("Single link SSE for given params = ", single_link_sse))
print(paste("Complete link SSE for given params = ", complete_link_sse))
# Next, lets evaluate visually by visualizing them
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=kmeans_result, main = "KMeans with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=single_link_result, main = "Single Link with 2 clusters", xlab = 'x', ylab = 'y')
plot(clustering_data$x, clustering_data$y, type='p', pch = '*', col=complete_link_result, main = "Complete link with 2 clusters", xlab = 'x', ylab = 'y')
# create a parameter grid
parameter_grid <- expand.grid(size=c(4, 8, 16), decay = c(0.2, 0.5, 0.002, 0.005))
# perform grid search, predict on test set
nn_result <- alda_nn(x_train = clf_train_df[,-5], x_test = clf_test_df[, -5],
y_train = clf_train_df[,5],
parameter_grid = parameter_grid)
print(paste("Accuracy of my NN model = ", confusionMatrix(nn_result[[2]], clf_test_df[, 5])$overall['Accuracy']))
print("Confusion Matrix of my NN model: ")
print(confusionMatrix(nn_result[[2]], clf_test_df[, 5])$table)
sample_n(data, 10)
randomRows(data, 10)
randomRows = function(df,n){
return(df[sample(nrow(df),n),])
}
randomRows(data, 10)
data
data = read.csv('./Medium_Clean.csv', stringsAsFactors = FALSE);
getwd()
setwd(paste("/Users/bharatsinha/", "code/medium", sep = ""))
getwd()
data = read.csv('./Medium_Clean.csv', stringsAsFactors = FALSE);
typeof(data)
data[1]
data[, 1]
data[1, ]
a = data.frame()
typeof(a)
randomRows(data, 100)
randomRows = function(df,n){
return(df[sample(nrow(df),n),])
}
randomRows(data, 100)
randomRows(data, 1)
data = randomRows(data, 1000)
contentList = data.frame();
startIndex = 2001;
startIndex = 1;
endIndex = 1000;
for (i in seq(1, 1000)) {
url = as.vector(data[i, 'url']);
contentList[i, 'url'] = url;
texts = tryCatch({
webpage = read_html(url);
pTags = html_nodes(webpage, 'p');
pText = html_text(pTags);
language = textcat::textcat(pText);
if (!is.na(language[1]) && language[1] == 'english') {
print(paste("Success on:", i, url))
logger(paste("Success on:", i, url))
pText;
} else {
print(paste("Fail on:", i, url, "Language"))
logger(paste("Fail on:", i, url, "Language"));
msg = NA;
msg;
}
}, error = function(error_condition) {
print(paste("Fail on:", i, url, "HTTP ERROR", error_condition))
logger(paste("Fail on:", i, url, "HTTP ERROR", error_condition));
msg = NA;
msg;
});
contentList[i, 'content'] = paste(texts, sep = " ", collapse = " ");
if (i%%1000 == 0) {
endIndex = i;
new_data=merge(data[startIndex:endIndex,], contentList, by.y = "url");
new_data_filter_1 = dplyr::filter(new_data, new_data['content'] != 'NA')
print("Writing the file");
write.csv(new_data_filter_1, file = paste("Data", "_", startIndex,"_", endIndex, ".csv", sep = ""))
startIndex = i+1;
contentList = data.frame();
}
}
logger = function(message) {
write(message, file="log.txt", append=TRUE)
}
for (i in seq(1, 1000)) {
url = as.vector(data[i, 'url']);
contentList[i, 'url'] = url;
texts = tryCatch({
webpage = read_html(url);
pTags = html_nodes(webpage, 'p');
pText = html_text(pTags);
language = textcat::textcat(pText);
if (!is.na(language[1]) && language[1] == 'english') {
print(paste("Success on:", i, url))
logger(paste("Success on:", i, url))
pText;
} else {
print(paste("Fail on:", i, url, "Language"))
logger(paste("Fail on:", i, url, "Language"));
msg = NA;
msg;
}
}, error = function(error_condition) {
print(paste("Fail on:", i, url, "HTTP ERROR", error_condition))
logger(paste("Fail on:", i, url, "HTTP ERROR", error_condition));
msg = NA;
msg;
});
contentList[i, 'content'] = paste(texts, sep = " ", collapse = " ");
if (i%%1000 == 0) {
endIndex = i;
new_data=merge(data[startIndex:endIndex,], contentList, by.y = "url");
new_data_filter_1 = dplyr::filter(new_data, new_data['content'] != 'NA')
print("Writing the file");
write.csv(new_data_filter_1, file = paste("Data", "_", startIndex,"_", endIndex, ".csv", sep = ""))
startIndex = i+1;
contentList = data.frame();
}
}
install.packages('rvest');
install.packages('textcat');
require('rvest');
require('textcat');
install.packages('dplyr');
install.packages("dplyr")
require('dplyr')
randomRows = function(df,n){
return(df[sample(nrow(df),n),])
}
data = randomRows(data, 1000)
contentList = data.frame();
startIndex = 1;
endIndex = 1000;
for (i in seq(1, 1000)) {
url = as.vector(data[i, 'url']);
contentList[i, 'url'] = url;
texts = tryCatch({
webpage = read_html(url);
pTags = html_nodes(webpage, 'p');
pText = html_text(pTags);
language = textcat::textcat(pText);
if (!is.na(language[1]) && language[1] == 'english') {
print(paste("Success on:", i, url))
#logger(paste("Success on:", i, url))
pText;
} else {
print(paste("Fail on:", i, url, "Language"))
#logger(paste("Fail on:", i, url, "Language"));
msg = NA;
msg;
}
}, error = function(error_condition) {
print(paste("Fail on:", i, url, "HTTP ERROR", error_condition))
#logger(paste("Fail on:", i, url, "HTTP ERROR", error_condition));
msg = NA;
msg;
});
contentList[i, 'content'] = paste(texts, sep = " ", collapse = " ");
if (i%%1000 == 0) {
endIndex = i;
new_data=merge(data[startIndex:endIndex,], contentList, by.y = "url");
new_data_filter_1 = dplyr::filter(new_data, new_data['content'] != 'NA')
print("Writing the file");
write.csv(new_data_filter_1, file = paste("Data", "_", startIndex,"_", endIndex, ".csv", sep = ""))
startIndex = i+1;
contentList = data.frame();
}
}
data_new = read.csv('./Data_1_1000.csv', stringsAsFactors = FALSE);
data_new['content']
tfidf = TfIdf$new(smooth_idf = TRUE, norm = c('l1', 'l2', 'none'), sublinear_tf = FALSE)
install.packages('text2vec');
tfidf = TfIdf$new(smooth_idf = TRUE, norm = c('l1', 'l2', 'none'), sublinear_tf = FALSE)
require('text2vec');
tfidf = TfIdf$new(smooth_idf = TRUE, norm = c('l1', 'l2', 'none'), sublinear_tf = FALSE)
tfidf$fit_transform(x)
tfidf$fit_transform(data_new['content'])
tfidf$fit_transform(matrix(data_new['content']))
data[1,]
data[1,8]
data[1,9]
data[1,10]
View(data_new)
View(data_new)
View(data_new)
